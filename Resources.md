### Deep Learning Resources

* [Chris Olah's Blog on LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Edwin Chen's Blog on LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)
* [Andrej Karpathy's Lecture](https://www.youtube.com/watch?v=iX5V1WpxxkY)
* [GRU](http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf)
* [Demo](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html)
* [Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)
* [Activation Functions](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)
* [MSE](https://en.wikipedia.org/wiki/Mean_squared_error)
* [Cross Entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf)
* [Linear Combination](http://linear.ups.edu/html/section-LC.html)
* [Partial Derivatives](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)
* [Common Derivatives](http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf)
* [Learning Rate 1](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)
* [Learning Rate 2](http://cs231n.github.io/neural-networks-3/#loss)
* [Gradient Clipping](https://arxiv.org/abs/1211.5063)
* [Character Generation RNN](https://github.com/udacity/deep-learning/blob/master/intro-to-rnns/Anna_KaRNNa_Solution.ipynb)
* [Deep Learning Interactive Lessons by Jay](http://jalammar.github.io/)
* [Adaptive Learning](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)
* [Adaptive Gradient](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)
* [Systematic Evaluation of CNN](https://arxiv.org/abs/1606.02228)
* [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906v2)
* [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)
* [An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf)
* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555)
* [Efficient Backpropagation](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
* [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533)
* [Choosing Hyperparameter](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters)
* [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523)
* []()




