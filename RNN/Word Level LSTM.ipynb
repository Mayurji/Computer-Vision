{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('/Users/mayurjain/Desktop/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictonaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "words = tuple(set(text.split()))\n",
    "int2words = dict(enumerate(words))\n",
    "words2int = {ch: ii for ii, ch in int2words.items()}\n",
    "encoded = np.array([words2int[ch] for ch in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22291,    64, 12751, ..., 26993, 26372,  9320])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter',\n",
       " '1',\n",
       " 'Happy',\n",
       " 'families',\n",
       " 'are',\n",
       " 'all',\n",
       " 'alike;',\n",
       " 'every',\n",
       " 'unhappy',\n",
       " 'family',\n",
       " 'is',\n",
       " 'unhappy',\n",
       " 'in',\n",
       " 'its',\n",
       " 'own',\n",
       " 'way.',\n",
       " 'Everything',\n",
       " 'was',\n",
       " 'in',\n",
       " 'confusion',\n",
       " 'in',\n",
       " 'the',\n",
       " \"Oblonskys'\",\n",
       " 'house.',\n",
       " 'The',\n",
       " 'wife',\n",
       " 'had',\n",
       " 'discovered',\n",
       " 'that',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'was',\n",
       " 'carrying',\n",
       " 'on',\n",
       " 'an',\n",
       " 'intrigue',\n",
       " 'with',\n",
       " 'a',\n",
       " 'French',\n",
       " 'girl,',\n",
       " 'who',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'governess',\n",
       " 'in',\n",
       " 'their',\n",
       " 'family,',\n",
       " 'and',\n",
       " 'she',\n",
       " 'had',\n",
       " 'announced',\n",
       " 'to',\n",
       " 'her',\n",
       " 'husband',\n",
       " 'that',\n",
       " 'she',\n",
       " 'could',\n",
       " 'not',\n",
       " 'go',\n",
       " 'on',\n",
       " 'living',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'house',\n",
       " 'with',\n",
       " 'him.',\n",
       " 'This',\n",
       " 'position',\n",
       " 'of',\n",
       " 'affairs',\n",
       " 'had',\n",
       " 'now',\n",
       " 'lasted',\n",
       " 'three',\n",
       " 'days,',\n",
       " 'and',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'and',\n",
       " 'wife',\n",
       " 'themselves,',\n",
       " 'but',\n",
       " 'all',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'their',\n",
       " 'family',\n",
       " 'and',\n",
       " 'household,',\n",
       " 'were',\n",
       " 'painfully',\n",
       " 'conscious',\n",
       " 'of',\n",
       " 'it.',\n",
       " 'Every']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22291,    64, 12751,  2113, 27996, 24564,  8542, 14895, 15446,\n",
       "        1329, 24200, 15446,   583, 27299, 23086,  7807, 24824,  4254,\n",
       "         583, 23761,   583, 13503, 25506,  6062, 27506,   681,  6509,\n",
       "        3933,  3823, 13503, 19517,  4254,  9291,  8216, 19411,  7897,\n",
       "       29142,  5710, 10945,  1190, 19105,  6509, 20121,  5710,  9148,\n",
       "         583,  2862, 21000,  9753, 24839,  6509,  5701,  8902,  4763,\n",
       "       19517,  3823, 24839, 19151, 16340, 19983,  8216, 18492,   583,\n",
       "       13503, 25311, 16274, 29142,  7845, 25338, 15417,  3304, 14065,\n",
       "        6509,  4213, 26955,  3786, 25238,  9753, 16340,  5134, 13503,\n",
       "       19517,  9753,   681, 20645, 28851, 24564, 13503, 16501,  3304,\n",
       "        2862,  1329,  9753, 21430, 16307, 10526, 29218,  3304, 26837,\n",
       "        3617])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[22291    64 12751  2113 27996 24564  8542 14895 15446  1329]\n",
      " [27154   583 13503  9706  3304 22817 16978 26748  5436  2572]\n",
      " [ 6509  8168  6416 15588 26083 18586 15404  7755 13504  9385]\n",
      " [28597   583  5710  5442 15367 21086  6509 12400  8902 21436]\n",
      " [10662  6838 24715 13503  4988  2813 15859 12486 13503  4993]\n",
      " [11037  5710 11368  3304  5087 16892 27506 21187 21086  6509]\n",
      " [11188 26528  9753  8757  8902 13503 14557  3304 27893  9753]\n",
      " [13503  2891   302 23737 15203 21675 21058  3945 26993 23435]\n",
      " [13503  7894  1817  3304 19411  4988 18645  2852  1672  4254]\n",
      " [ 1190 19734   123 23061  2572  7056 10841 14938 27506 13485]]\n",
      "\n",
      "y\n",
      " [[   64 12751  2113 27996 24564  8542 14895 15446  1329 24200]\n",
      " [  583 13503  9706  3304 22817 16978 26748  5436  2572  1946]\n",
      " [ 8168  6416 15588 26083 18586 15404  7755 13504  9385 23950]\n",
      " [  583  5710  5442 15367 21086  6509 12400  8902 21436 15815]\n",
      " [ 6838 24715 13503  4988  2813 15859 12486 13503  4993 29142]\n",
      " [ 5710 11368  3304  5087 16892 27506 21187 21086  6509 23814]\n",
      " [26528  9753  8757  8902 13503 14557  3304 27893  9753  1046]\n",
      " [ 2891   302 23737 15203 21675 21058  3945 26993 23435 24564]\n",
      " [ 7894  1817  3304 19411  4988 18645  2852  1672  4254 13503]\n",
      " [19734   123 23061  2572  7056 10841 14938 27506 13485 29142]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        ## TODO: Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        ## TODO: pass x through a droupout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(29230, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=29230, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = CharRNN(words, n_hidden=512, n_layers=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 8.6368... Val Loss: 8.6551\n",
      "Epoch: 1/10... Step: 20... Loss: 8.3034... Val Loss: 8.2754\n",
      "Epoch: 2/10... Step: 30... Loss: 7.9721... Val Loss: 8.0434\n",
      "Epoch: 2/10... Step: 40... Loss: 7.8390... Val Loss: 7.8640\n",
      "Epoch: 3/10... Step: 50... Loss: 7.6038... Val Loss: 7.7103\n",
      "Epoch: 3/10... Step: 60... Loss: 7.4621... Val Loss: 7.5508\n",
      "Epoch: 3/10... Step: 70... Loss: 7.3454... Val Loss: 7.4271\n",
      "Epoch: 4/10... Step: 80... Loss: 7.1729... Val Loss: 7.3142\n",
      "Epoch: 4/10... Step: 90... Loss: 7.0925... Val Loss: 7.2414\n",
      "Epoch: 5/10... Step: 100... Loss: 6.9424... Val Loss: 7.1809\n",
      "Epoch: 5/10... Step: 110... Loss: 6.7779... Val Loss: 7.1187\n",
      "Epoch: 5/10... Step: 120... Loss: 6.7744... Val Loss: 7.0846\n",
      "Epoch: 6/10... Step: 130... Loss: 6.5740... Val Loss: 7.1443\n",
      "Epoch: 6/10... Step: 140... Loss: 6.5954... Val Loss: 7.0446\n",
      "Epoch: 7/10... Step: 150... Loss: 6.4174... Val Loss: 7.0076\n",
      "Epoch: 7/10... Step: 160... Loss: 6.4295... Val Loss: 7.0014\n",
      "Epoch: 8/10... Step: 170... Loss: 6.2785... Val Loss: 7.0065\n",
      "Epoch: 8/10... Step: 180... Loss: 6.2084... Val Loss: 6.9897\n",
      "Epoch: 8/10... Step: 190... Loss: 6.1595... Val Loss: 6.9865\n",
      "Epoch: 9/10... Step: 200... Loss: 6.0392... Val Loss: 7.0309\n",
      "Epoch: 9/10... Step: 210... Loss: 5.9861... Val Loss: 7.0213\n",
      "Epoch: 10/10... Step: 220... Loss: 5.7991... Val Loss: 7.0547\n",
      "Epoch: 10/10... Step: 230... Loss: 5.7237... Val Loss: 7.1508\n",
      "Epoch: 10/10... Step: 240... Loss: 5.7272... Val Loss: 7.1069\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "# you may change cuda to True if you plan on using a GPU!\n",
    "# also, if you do, please INCREASE the epochs to 25\n",
    "train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ' '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A n n a that he had come to him. She was at his own face of his head and at the same time as to his face was a of the same time as though the most of the most old man and his face of the same time as though he had been his own And he was not in the first but he was a long in his own But the most of which he had been on the other same as though they were were not a man in a same The time he had been in the same same as though his were were a man of the same same time with his eyes of the same were his eyes and a old new He had not been his own The first time for his head but was not to his head and to be at his own own The position in the first for his face of his own own and his own feeling of his own own his eyes and his face was a long of his old and was at the same time of the same He was his own face that had not been for his\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 200, prime='Anna', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
